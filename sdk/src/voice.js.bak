import { MurfFalconClient } from './utils/murf.js';
import { Store } from './store.js';

export class VoiceHandler {
    constructor() {
        this.recognition = null;

        // Get Murf API Key from config or environment
        const config = Store.getState().config || {};
        const envKey = (typeof import.meta !== 'undefined' && import.meta.env?.VITE_MURF_API_KEY);
        const murfKey = config.murfApiKey || envKey || 'ap2_6c04b66d-d001-415c-a8e8-7510c9262174';

        this.murfClient = new MurfFalconClient(murfKey);
        this.isListening = false;
        this.isSpeaking = false;
        this.silenceTimer = null;
        this.silenceThreshold = 1500; // 1.5 seconds of silence
        this.transcript = '';
        this.onResponseCallback = null;
        this.onStatusChange = null;
        this.browserInfo = this.detectBrowser();
    }

    /**
     * Detect browser and its capabilities
     */
    detectBrowser() {
        const ua = navigator.userAgent;
        const isChrome = /Chrome/.test(ua) && /Google Inc/.test(navigator.vendor);
        const isSafari = /Safari/.test(ua) && /Apple Computer/.test(navigator.vendor);
        const isFirefox = /Firefox/.test(ua);
        const isEdge = /Edg/.test(ua);

        const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
        const hasSpeechRecognition = !!SpeechRecognition;
        const hasSpeechSynthesis = 'speechSynthesis' in window;

        return {
            isChrome,
            isSafari,
            isFirefox,
            isEdge,
            hasSpeechRecognition,
            hasSpeechSynthesis,
            name: isChrome ? 'Chrome' : isSafari ? 'Safari' : isFirefox ? 'Firefox' : isEdge ? 'Edge' : 'Unknown'
        };
    }

    /**
     * Start voice mode
     * @param {Function} onResponse - Callback to get AI response
     * @param {Function} onStatusChange - Callback for status updates
     */
    async start(onResponse, onStatusChange) {
        this.onResponseCallback = onResponse;
        this.onStatusChange = onStatusChange;

        // Initialize Murf Falcon Connection early (within user gesture)
        try {
            console.log('[VoiceHandler] Initializing Murf link...');
            await this.murfClient.connect();
        } catch (err) {
            console.warn('[VoiceHandler] Pre-connection failed, will retry on speak:', err);
        }

        // Check browser support with detailed error messages
        const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;

        if (!SpeechRecognition) {
            const errorMessage = this.getBrowserSpecificError();
            throw new Error(errorMessage);
        }

        // Check for HTTPS (required for mic access in most browsers)
        if (location.protocol !== 'https:' && location.hostname !== 'localhost' && location.hostname !== '127.0.0.1') {
            throw new Error('Voice mode requires HTTPS or localhost. Please use a secure connection.');
        }

        this.recognition = new SpeechRecognition();

        // Configure for continuous recognition
        this.recognition.continuous = true;
        this.recognition.interimResults = true;
        this.recognition.lang = 'en-US';
        this.recognition.maxAlternatives = 1;

        // Event handlers
        this.recognition.onresult = this.handleSpeechResult.bind(this);
        this.recognition.onend = this.handleRecognitionEnd.bind(this);
        this.recognition.onerror = this.handleError.bind(this);
        this.recognition.onstart = () => {
            console.log('[VoiceHandler] Recognition started successfully');
            this.updateStatus('listening');
        };

        try {
            console.log(`[VoiceHandler] Starting voice mode in ${this.browserInfo.name}`);
            this.recognition.start();
            this.isListening = true;
        } catch (error) {
            console.error('Failed to start recognition:', error);
            throw new Error(`Failed to start voice mode: ${error.message}. Please ensure microphone permissions are granted.`);
        }
    }

    /**
     * Get browser-specific error message
     */
    getBrowserSpecificError() {
        if (this.browserInfo.isFirefox) {
            return 'Voice mode is not supported in Firefox. Please use Chrome, Edge, or Safari for voice features.';
        } else if (this.browserInfo.isSafari) {
            return 'Voice mode has limited support in Safari. For best experience, please use Chrome or Edge.';
        } else {
            return 'Speech recognition is not supported in this browser. Please use Chrome, Edge, or Safari.';
        }
    }

    /**
     * Handle speech recognition results
     */
    handleSpeechResult(event) {
        // NATURAL INTERRUPTION: If user starts speaking while AI is talking, stop the AI
        if (this.isSpeaking) {
            console.log('[VoiceHandler] User interrupted AI speech');
            this.interrupt();
        }

        // Clear silence timer (user is speaking)
        clearTimeout(this.silenceTimer);
        this.updateStatus('speaking');

        // Get latest transcript
        const result = event.results[event.results.length - 1];
        const transcript = result[0].transcript;

        if (result.isFinal) {
            this.transcript += transcript + ' ';

            // Start silence detection
            this.silenceTimer = setTimeout(() => {
                this.processSpeech();
            }, this.silenceThreshold);

            this.updateStatus('listening');
        }
    }

    /**
     * Process speech after silence detected
     */
    async processSpeech() {
        if (!this.transcript.trim()) return;

        const userMessage = this.transcript.trim();
        this.transcript = '';

        this.updateStatus('processing');

        try {
            // Get AI response
            if (this.onResponseCallback) {
                const aiResponse = await this.onResponseCallback(userMessage);
                await this.speak(aiResponse);
            }
        } catch (error) {
            console.error('Error processing speech:', error);
            this.updateStatus('listening');
        }
    }

    /**
     * Text-to-speech using Murf Falcon WebSocket
     */
    async speak(text) {
        // PAUSE RECOGNITION WHILE AI IS SPEAKING (prevents feedback)
        if (this.recognition && this.isListening) {
            try {
                this.recognition.stop();
            } catch (error) {
                console.log('Recognition already stopped:', error);
            }
        }

        this.isSpeaking = true;
        this.updateStatus('ai-speaking');

        try {
            // Try Murf first (unless we've already switched to fallback)
            if (!this.useFallbackTTS) {
                // Set up status listener for the murf client
                this.murfClient.onStatusChange = (status) => {
                    if (status === 'idle') {
                        this.handleSpeechEnd();
                    } else if (status === 'error') {
                        // Murf failed, switch to browser TTS
                        console.warn('[VoiceHandler] Murf error detected, switching to browser TTS');
                        this.useFallbackTTS = true;
                        this.handleSpeechEnd();
                    }
                };

                // Attempt Murf synthesis
                await this.murfClient.speak(text);

                // If we reach here and no audio started playing within 3 seconds, assume Murf failed
                setTimeout(() => {
                    if (this.isSpeaking && !this.murfClient.isPlaying) {
                        console.warn('[VoiceHandler] Murf timeout, switching to browser TTS');
                        this.useFallbackTTS = true;
                        this.speakWithBrowserTTS(text);
                    }
                }, 3000);
            } else {
                // Use browser TTS fallback
                await this.speakWithBrowserTTS(text);
            }
        } catch (error) {
            console.error('[VoiceHandler] Murf speak error:', error);
            // Fallback to browser TTS
            console.log('[VoiceHandler] Falling back to browser TTS');
            this.useFallbackTTS = true;
            await this.speakWithBrowserTTS(text);
        }
    }

    /**
     * Speak using browser's native TTS (fallback)
     */
    async speakWithBrowserTTS(text) {
        try {
            this.browserTTS.onStatusChange = (status) => {
                if (status === 'idle') {
                    this.handleSpeechEnd();
                }
            };
            await this.browserTTS.speak(text);
        } catch (error) {
            console.error('[VoiceHandler] Browser TTS error:', error);
            this.handleSpeechEnd();
        }
    }

    /**
     * Handle end of speech
     */
    handleSpeechEnd() {
        this.isSpeaking = false;
        this.updateStatus('listening');

        // RESUME RECOGNITION AFTER AI FINISHES SPEAKING
        if (this.isListening && this.recognition) {
            setTimeout(() => {
                try {
                    // Check if still listening mode
                    if (this.isListening && !this.isSpeaking) {
                        this.recognition.start();
                    }
                } catch (error) {
                    console.error('Failed to restart recognition:', error);
                }
            }, 500); // 500ms delay to avoid picking up tail end of speech
        }
    }

    /**
     * Interrupt AI speech (when user starts speaking)
     */
    interrupt() {
        if (this.isSpeaking) {
            this.murfClient.flushBuffer();
            this.browserTTS.interrupt();
            this.handleSpeechEnd();
        }
    }

    /**
     * Stop voice mode completely
     */
    stop() {
        this.isListening = false;
        this.isSpeaking = false;
        clearTimeout(this.silenceTimer);

        if (this.recognition) {
            try {
                this.recognition.stop();
            } catch (e) { }
            this.recognition = null;
        }

        this.murfClient.disconnect();
        this.updateStatus('idle');
    }

    /**
     * Handle recognition end (auto-restart)
     */
    handleRecognitionEnd() {
        // Auto-restart if still in voice mode
        if (this.isListening && this.recognition) {
            try {
                this.recognition.start();
            } catch (error) {
                console.error('Failed to restart recognition:', error);
            }
        }
    }

    /**
     * Handle recognition errors
     */
    handleError(event) {
        console.error('Speech recognition error:', event.error);

        // Don't stop on no-speech or aborted errors
        if (event.error === 'no-speech' || event.error === 'aborted') {
            return;
        }

        // For other errors, try to restart
        if (this.isListening) {
            setTimeout(() => {
                try {
                    this.recognition.start();
                } catch (error) {
                    console.error('Failed to restart after error:', error);
                }
            }, 1000);
        }
    }

    /**
     * Update status and notify callback
     */
    updateStatus(status) {
        if (this.onStatusChange) {
            this.onStatusChange(status);
        }
    }
}
